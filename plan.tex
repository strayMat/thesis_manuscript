Thesis plan

1.1. Overview and contributions	5
1.2. Definitions	5
1.2.1. Various types of data: Real World Data and observational data	5
1.2.1.1. Real world data refer to routinely collected data : gradients and distinction between research data and opportunistic data collection.	5
1.2.1.2. Observational data vs interventional data	5
1.2.2. Various sources of data:	5
1.2.2.1. Claims	5
1.2.2.2. EHRs and Hospital Information System	5
1.2.2.3. Clinical Data Warehouse	5
1.2.3. Primary Use / Secondary use	5
1.2.4. Statistical Learning Framework	6
1.2.4.1. General framework	6
1.2.4.2. Model selection	6
1.2.4.3. Statistical estimators	6
1.2.5. Potential Outcome Framework	6
1.2.5.1. General framework	6
1.2.5.2. Estimands	6
1.2.5.3. Causal assumptions	6
1.2.5.4. Causal estimators	6
2.1. Motivation	7
2.1.1. The increased collection of clinical data needs to be structured to be useful.	7
2.1.2. What is the state of routine care data collection and what usage in France ?	7
2.2. Background	7
2.2.1. Healthcare data collection system, an influence from the local organization of the healthcare actors.	7
2.2.2. The case of France.	7
2.3. Methods	7
2.3.1. Interviews and study coverage	7
2.3.1.1. Semi-structured interviews	7
2.3.1.2. Regional and university hospitals in France: different levels of maturity	7
2.3.2. A classification of observational studies	7
2.4. Results	7
2.4.1. Governance	7
2.4.1.1. Initiation and actors	7
2.4.1.2. Federating potentia	7
2.4.1.3. In-house solution development vs industrialization	7
2.4.2. Management of studies	7
2.4.3. Transparency	7
2.4.4. Data	8
2.4.4.1. Strong dependance to the HIS.	8
2.4.4.2. Categories of Data	8
2.4.4.3. Data reuse: research
CDW are used for monitoring and management	8
2.4.4.4. Strong interest for CDW in the context of care	8
2.4.5. Technical architecture	8
2.4.6. Data quality, standard formats	8
2.4.6.1.1. Quality tools	8
2.4.6.1.2. Standard format	8
2.4.6.1.3. Documentation	8
2.5. Recommandations	8
2.5.1. Governance	8
2.5.2. Transparency	8
2.5.3. Data	8
2.5.4. Technical architecture	8
2.5.5. Data quality, standard formats	8
2.6. Conclusion	8
3.1. Motivation :	9
3.1.1. There is a strong focus on predictive tasks from EHRs	9
3.1.2. EHRs data is complex (time, high cardinality, multi-modality)	9
3.1.3. What are the most impactful algorithms for predictive tasks from structured EHR ?	9
3.2. Methods	9
3.2.1. Task definitions	9
3.2.1.1. Length Of Stay	9
3.2.1.2. Diagnosis Prediction	9
3.2.1.3. Major Adverse Cardiovascular Events	9
3.2.2. Predictive pipelines	9
3.2.2.1. Demographic features	9
3.2.2.2. Count encoding with event features	9
3.2.2.3. Static Embeddings of event features	9
3.2.2.4. Transformer based	9
3.2.3. Evaluation pipeline	9
3.3. Results	10
3.3.1. The performance-sample trade-offs	10
3.4. Conclusions	10
4.1. Motivation : Healthcare is concerned with decision making, not mere prediction	11
4.1.1. Predictive medicine currently suffers from biases (shortcuts, population shifts). Racial, gender and under-served population biases raise concern on fairness.	11
4.1.2. The key ingredient to ground data-driven decision making is causal thinking	11
4.1.3. The relevant concepts for causal inference are scattered in different literatures. A dedicated exposition to time-varying data in EHRs would help practitioners and data scientists that study them.	11
4.2. Step-by-step framework for robust decision making from EHR data	11
4.2.1. Robust study design to avoid biases: Framing the question	11
4.2.1.1. PICOT	11
4.2.1.2. Selection Bias	11
4.2.1.3. Immortal time bias	11
4.2.2. Is the dataset sufficient to inform on the intervention: identification	11
4.2.2.1. Causal graph	11
4.2.3. Computing the causal effect of interest: Estimation	11
4.2.3.1. Confounders extractions	11
4.2.3.2. Confounders aggregation	11
4.2.3.3. Causal estimators	11
4.2.3.4. Nuisance estimators	11
4.2.4. Assessing the robustness of the hypothesis: Vibration or sensitivity analysis	11
4.2.5. Heterogeneity of treatment	12
4.3. Application on MIMIC-IV	12
4.3.1. Emulated trial: Effect of albumin in combination with crystalloids compared to crystalloids alone on 28-day mortality in sepsis patients	12
4.3.1.1. Choice of the trial	12
4.3.1.2. Known effects	12
4.3.2. Framing the question	12
4.3.3. Identification	12
4.3.4. Estimation	12
4.3.4.1. Confounders extractions	12
4.3.4.2. Confounders aggregation	12
4.3.4.3. Causal estimators	12
4.3.4.4. Nuisance estimators	12
4.3.5. Vibration analysis	12
4.3.5.1. Varying estimation choices:	12
4.3.5.2. Varying inclusion criteria: illustration of immortal time bias	12
4.3.6. Heterogeneity of treatment	12
4.4. Discussion	12
5.1. Motivation	13
5.1.1. Extending prediction to prescription requires causal model selection	13
5.1.2. Illustration: the best predictor may not estimate best causal effects	13
5.1.3. Prior work: model selection for outcome modeling (g-computation)	13
5.2. Methods	13
5.2.1. Notations	13
5.2.2. Model-selection risks, oracle and feasible	13
5.2.2.1. Causal model selection	13
5.2.2.2. The τ -risk: an oracle error risk	13
5.2.2.3. Feasible error risks	13
5.2.3. Estimation and model selection procedure	13
5.3. Theory: Links between feasible and oracle risks	13
5.3.1. Upper bound of τ -risk with μ-riskIP W	13
5.3.2. Reformulation of the R-risk as reweighted τ -risk	13
5.3.3. Interesting special cases	13
5.4. Empirical Study	13
5.4.1. Caussim: Extensive simulation settings	13
5.4.1.1. Data Generation Process	13
5.4.1.2. Family of candidate estimators	13
5.4.2. Semi-simulated datasets	13
5.4.2.1. Datasets	13
5.4.2.2. Family of candidate estimators	13
5.4.2.3. Nuisance estimators	13
5.4.3. Measuring overlap between treated and non treated	13
5.4.4. Empirical results: factors driving good model selection across datasets	13
5.4.4.1. The R-risk is the best metric	13
5.4.4.2. Model selection is harder in settings of low population overlap	13
5.4.4.3. Nuisances can be estimated on the same data as outcome models	13
5.4.4.4. Use 90% of the data to estimate outcome models, 10% to select them	14
5.5. Discussion and conclusion	14

Introduction
Overview and contributions

Definitions
Various types of data: Real World Data and observational data
Real world data refer to routinely collected data : gradients and distinction between research data and opportunistic data collection.
Observational data vs interventional data
Various sources of data:
Claims
Billing, advantages (space and time coverage, scale, structure), disadvantages (not clinic, no exam results, few measures, heterogeneity of collection, )
EHRs and Hospital Information System
Increasing informatization
EHR in the center
Other applications part of HIS
Clinical Data Warehouse
An infrastructure is needed to pool data from one or more medical information systems.

Primary Use / Secondary use
Primary usages directly serve one patient care.
Secondary usages do not concern directly the care and support of one patient: research, quality or management indicators, billings.
Mixt usages such as learning algorithms

Statistical Learning Framework
General framework
Model selection
Statistical estimators
Potential Outcome Framework
General framework
Estimands
Causal assumptions
Causal estimators

Potential and challenges of CDWs, a case study in France
Motivation
The increased collection of clinical data needs to be structured to be useful.

What is the state of routine care data collection and what usage in France ?
Background
Healthcare data collection system, an influence from the local organization of the healthcare actors.


The case of France.
Methods
Interviews and study coverage
Semi-structured interviews
Regional and university hospitals in France: different levels of maturity
A classification of observational studies
Outcome frequency, Population characterization, Risk factors, Treatment effect, Development of diagnostic or prognostic algorithms, Medical informatics
Results
Governance
Initiation and actors
Federating potentia
In-house solution development vs industrialization

Management of studies
Scientific committee and project follow-up platform
Transparency
Uneven public reference on hospital websites of ongoing studies.
Data
Strong dependance to the HIS.
Categories of Data
Data reuse: research
CDW are used for monitoring and management
Strong interest for CDW in the context of care
Technical architecture
Three layer : Data preprocessing (acquisition and normalization), storage, exposure
Datalab
Data quality, standard formats
Quality tools
Standard format
Documentation
Recommandations
Governance
CDW becomes an essential component of data management
Resources specific to the warehouse are rare and only project-based
Multi-layered governance
Transparency
Public registration of comparative observational study protocols
Data
Technical architecture
Data quality, standard formats

Conclusion
Useful representations and predictive algorithms for EHRs, benchmarking three tasks
Motivation :
There is a strong focus on predictive tasks from EHRs
In the literature

Within french CDW (23 % of studies, just after population definitions)
EHRs data is complex (time, high cardinality, multi-modality)
In the literature
Within french CDW

What are the most impactful algorithms for predictive tasks from structured EHR ?
A wealth of methods, but a lack of insights on the advantages and inconvenience for specific problems and resources.
Methods
Task definitions
Length Of Stay
Diagnosis Prediction
Major Adverse Cardiovascular Events
Predictive pipelines
Demographic features
Count encoding with event features
Static Embeddings of event features
Transformer based
Evaluation pipeline

Results
The performance-sample trade-offs
Conclusions

Prediction is not all we need: Causal analysis of EHRs to ground decision making
Motivation : Healthcare is concerned with decision making, not mere prediction
Predictive medicine currently suffers from biases (shortcuts, population shifts). Racial, gender and under-served population biases raise concern on fairness.
The key ingredient to ground data-driven decision making is causal thinking
The relevant concepts for causal inference are scattered in different literatures. A dedicated exposition to time-varying data in EHRs would help practitioners and data scientists that study them.
Step-by-step framework for robust decision making from EHR data
Robust study design to avoid biases: Framing the question
PICOT
Selection Bias
Immortal time bias
Is the dataset sufficient to inform on the intervention: identification
Causal graph
Computing the causal effect of interest: Estimation
Confounders extractions
Confounders aggregation
Causal estimators
Nuisance estimators
Assessing the robustness of the hypothesis: Vibration or sensitivity analysis

Sensitivity vs robustness
Heterogeneity of treatment
Interest of HTE
How to do HTE ? Final regression analysis vs other methods (eg. RATE)?
Application on MIMIC-IV
Emulated trial: Effect of albumin in combination with crystalloids compared to crystalloids alone on 28-day mortality in sepsis patients
Choice of the trial
Known effects
Framing the question
Identification
Covariates and dag
Estimation
Confounders extractions
Confounders aggregation
Causal estimators
Nuisance estimators
Vibration analysis
Varying estimation choices:
Varying inclusion criteria: illustration of immortal time bias
Heterogeneity of treatment
Discussion
How to select predictive models for causal inference ?
Motivation
Extending prediction to prescription requires causal model selection
Illustration: the best predictor may not estimate best causal effects
Prior work: model selection for outcome modeling (g-computation)
Methods
Notations
Model-selection risks, oracle and feasible
Causal model selection
The τ -risk: an oracle error risk
Feasible error risks
Estimation and model selection procedure
Theory: Links between feasible and oracle risks
Upper bound of τ -risk with μ-riskIP W
Reformulation of the R-risk as reweighted τ -risk
Interesting special cases
Empirical Study
Caussim: Extensive simulation settings
Data Generation Process
Family of candidate estimators
Semi-simulated datasets
Datasets
Family of candidate estimators
Nuisance estimators
Measuring overlap between treated and non treated
Empirical results: factors driving good model selection across datasets
The R-risk is the best metric
Model selection is harder in settings of low population overlap
Nuisances can be estimated on the same data as outcome models
Use 90\% of the data to estimate outcome models, 10\% to select them
Discussion and conclusion




Conclusion


